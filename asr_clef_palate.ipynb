{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "\n",
    "from transformers import TFWav2Vec2Model, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, TFWav2Vec2ForCTC\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import librosa as lb\n",
    "from librosa.effects import trim\n",
    "import librosa.display\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File_Path Speaker         Transcription Session clean_transcript\n",
      "0     03M_1     03M  1 2 3 4 5 6 7 8 9 10       1      12345678910\n",
      "1     03M_2     03M                   ata       2              ata\n",
      "2     03M_3     03M                   ana       3              ana\n",
      "3     03M_4     03M                   ara       4              ara\n",
      "4     03M_5     03M                  atha       5             atha\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"dataset/\"\n",
    "metadata = \"Datasets.csv\"\n",
    "\n",
    "audio_directory = \"dataset/\"\n",
    "\n",
    "# Create a dataframe for the transcript\n",
    "dataframe = pd.read_csv(metadata)\n",
    "\n",
    "# Preprocess transcript\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove non-alphanumeric characters (except spaces)\n",
    "    text = text.replace(\" \", \"\")  # Remove all whitespace\n",
    "    return text\n",
    "\n",
    "dataframe['clean_transcript'] = dataframe['Transcription'].apply(preprocess_text)\n",
    "\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file_path': 'dataset/01F_1.wav', 'transcript': '12345678910'}, {'file_path': 'dataset/01F_3.wav', 'transcript': 'ana'}, {'file_path': 'dataset/01F_4.wav', 'transcript': 'ara'}]\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess audio and connect to transcripts\n",
    "def combine_audio_with_transcript(directory, dataframe):\n",
    "    audio_data = []    \n",
    "     # Iterate over each row in the DataFrame\n",
    "    for index, row in dataframe.iterrows():\n",
    "        file_name = row['File_Path']  # Get the file name from the CSV (without .wav)\n",
    "        transcript = row['clean_transcript']  # Get the transcript\n",
    "        \n",
    "        # Construct the full file path by combining directory and file name with .wav extension\n",
    "        file_path = os.path.join(directory, f\"{file_name}.wav\")\n",
    "        \n",
    "        # Check if the file exists in the audio directory\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                # Append the processed data along with the transcript\n",
    "                audio_data.append({\n",
    "                    \"file_path\": file_path,\n",
    "                    \"transcript\": transcript,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    return audio_data\n",
    "\n",
    "# Preprocess audio files and connect to transcripts\n",
    "audio_data_with_transcripts = combine_audio_with_transcript(audio_directory, dataframe)\n",
    "print(audio_data_with_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get Dataset from the folder\n",
    "\n",
    "# prepared_audio_data = []\n",
    "\n",
    "# # Dataset file path\n",
    "# dataset_file_path = [item['file_path'] for item in audio_data_with_transcripts]\n",
    "\n",
    "# # Transcript\n",
    "# dataset_transcript = [item['transcript'] for item in audio_data_with_transcripts]\n",
    "\n",
    "# max_length = 0\n",
    "# sample_rate = 16000\n",
    "# num_mels = 128\n",
    "\n",
    "# try:\n",
    "#     for file_path in dataset_file_path:\n",
    "#         y, sr = librosa.load(file_path, sr=sample_rate, mono=True)\n",
    "#         temp_length = len(y)\n",
    "        \n",
    "#         if temp_length > max_length:\n",
    "#             max_length = temp_length\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "# try:\n",
    "#     for file_path in dataset_file_path:\n",
    "#         # Load the audio file\n",
    "#         y, sr = lb.load(file_path, sr=sample_rate, mono=True)\n",
    "        \n",
    "#         # Trim silent edges of the audio\n",
    "#         y, _ = lb.effects.trim(y)\n",
    "        \n",
    "#         # Normalize the audio\n",
    "#         y = lb.util.normalize(y)\n",
    "        \n",
    "#         # Pad the audio to the maximum length\n",
    "#         if len(y) < max_length:\n",
    "#             y = np.pad(y, (0, max_length - len(y)))\n",
    "#         else:\n",
    "#             y = y[:max_length]  # This line can be skipped if no truncation is desired\n",
    "        \n",
    "#         # Convert to Mel spectrogram\n",
    "#         mel_spec = lb.feature.melspectrogram(y=y, sr=sr, n_mels=num_mels)\n",
    "        \n",
    "#         # Convert to dB scale (log scale)\n",
    "#         mel_spec_db = lb.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "#         # Normalize the spectrogram between 0 and 1\n",
    "#         mel_spec_db = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "        \n",
    "#         # Append the processed Mel spectrogram to the list\n",
    "#         prepared_audio_data.append(mel_spec_db.T)  # Transpose to match the expected input shape\n",
    "\n",
    "#         print(f\"Loaded {file_path} with shape {y.shape}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TFWav2Vec2ForCTC has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2ForCTC: ['project_q.bias', 'project_hid.weight', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing TFWav2Vec2ForCTC from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFWav2Vec2ForCTC from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFWav2Vec2ForCTC were not initialized from the PyTorch model and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "    \"./wav2vec2-large-xlsr-53\",\n",
    "    from_pt=True\n",
    ")\n",
    "\n",
    "model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "    \"./wav2vec2-large-xlsr-53\",\n",
    "    from_pt=True\n",
    ")\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': <tf.Tensor: shape=(113960,), dtype=float32, numpy=\n",
      "array([-0.10316028, -0.1667561 , -0.13397591, ..., -0.01674847,\n",
      "       -0.00446125, -0.01840178], dtype=float32)>, 'labels': array([1, 0, 0, ..., 0, 0, 0]), 'input_length': <tf.Tensor: shape=(), dtype=int32, numpy=113960>, 'label_length': 1}\n",
      "{'input_values': <tf.Tensor: shape=(148146,), dtype=float32, numpy=\n",
      "array([0.13108213, 0.21958746, 0.18175448, ..., 0.08491193, 0.09992137,\n",
      "       0.09167296], dtype=float32)>, 'labels': array([2, 0, 0, ..., 0, 0, 0]), 'input_length': <tf.Tensor: shape=(), dtype=int32, numpy=148146>, 'label_length': 1}\n",
      "{'input_values': <tf.Tensor: shape=(148857,), dtype=float32, numpy=\n",
      "array([-0.00935751, -0.01834278, -0.03925886, ...,  0.12503275,\n",
      "        0.166711  ,  0.10804716], dtype=float32)>, 'labels': array([3, 0, 0, ..., 0, 0, 0]), 'input_length': <tf.Tensor: shape=(), dtype=int32, numpy=148857>, 'label_length': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer.fit_on_texts(labels['transcript'] for labels in audio_data_with_transcripts)\n",
    "\n",
    "max_audio_length = 0\n",
    "for sample in audio_data_with_transcripts:\n",
    "    speech, _ = librosa.load(sample['file_path'], sr=16000)\n",
    "    audio_length = len(speech)  # Length of the audio file (in samples)\n",
    "    if audio_length > max_audio_length:\n",
    "        max_audio_length = audio_length\n",
    "\n",
    "\n",
    "def preprocess(audio_data_with_transcripts, sampling_rate=16000):\n",
    "    speech, _ = librosa.load(audio_data_with_transcripts['file_path'], sr=sampling_rate)\n",
    "    input_values = extractor(speech, sampling_rate=sampling_rate, return_tensors=\"tf\").input_values\n",
    "\n",
    "    labels = tokenizer.texts_to_sequences([audio_data_with_transcripts['transcript']])\n",
    "    label_len = len(labels[0])\n",
    "\n",
    "    padded_labels = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        labels, maxlen=max_audio_length, padding='post'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_values\": input_values[0],\n",
    "        \"labels\": padded_labels[0],\n",
    "        \"input_length\": tf.shape(input_values[0])[0], # model_downsample_rate,  # adjust this\n",
    "        \"label_length\": label_len\n",
    "    }\n",
    "\n",
    "processed_data = [preprocess(data) for data in audio_data_with_transcripts]\n",
    "\n",
    "for data in processed_data:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator():\n",
    "    for item in processed_data:\n",
    "        yield {\n",
    "            'input_values': item['input_values'],\n",
    "            'labels': item['labels'],\n",
    "            'input_length': item['input_length'],\n",
    "            'label_length': item['label_length']\n",
    "        }\n",
    "\n",
    "output_signature = {\n",
    "    'input_values': tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
    "    'labels': tf.TensorSpec(shape=(max_audio_length,), dtype=tf.int32),\n",
    "    'input_length': tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "    'label_length': tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "}\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(data_generator, output_signature=output_signature)\n",
    "\n",
    "dataset = dataset.padded_batch(\n",
    "    batch_size=4,\n",
    "    padded_shapes={\n",
    "        'input_values': [None],  # Variable length\n",
    "        'labels': [max_audio_length],  # Fixed length for labels (adjust this as needed)\n",
    "        'input_length': [],  # Single scalar for the length of input sequence\n",
    "        'label_length': []  # Single scalar for the length of label sequence\n",
    "    },\n",
    "    padding_values={\n",
    "        'input_values': 0.0,  # Padding value for input\n",
    "        'labels': -100,  # Padding value for labels (if using CTC loss)\n",
    "        'input_length': 0,  # Padding value for input_length\n",
    "        'label_length': 0  # Padding value for label_length\n",
    "    },\n",
    "    drop_remainder=True  # This ensures no partial batches are included\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLossModel(tf.keras.Model):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Extract the data\n",
    "        input_values = data['input_values']\n",
    "        y_true = data['labels']\n",
    "        input_lengths = data['input_length']\n",
    "        label_lengths = data['label_length']\n",
    "\n",
    "        # Reshape input_lengths and label_lengths to match the expected shape\n",
    "        input_lengths = tf.reshape(input_lengths, (-1, 1))\n",
    "        label_lengths = tf.reshape(label_lengths, (-1, 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass through the base model\n",
    "            model_output = self.base_model(input_values, training=True)\n",
    "            logits = model_output.logits\n",
    "\n",
    "            # Calculate CTC loss\n",
    "            loss = tf.keras.backend.ctc_batch_cost(y_true, logits, input_lengths, label_lengths)\n",
    "\n",
    "        # Calculate gradients and apply them\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "# Compile the model\n",
    "CTCModel = CTCLossModel(model)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "CTCModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/attention/k_proj/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/attention/k_proj/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/attention/q_proj/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/attention/q_proj/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/attention/v_proj/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/attention/v_proj/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/attention/out_proj/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/attention/out_proj/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/layer_norm/gamma:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/layer_norm/beta:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/final_layer_norm/gamma:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.1/final_layer_norm/beta:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/attention/k_proj/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/attention/k_proj/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/attention/q_proj/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/attention/q_proj/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/attention/v_proj/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/attention/v_proj/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/attention/out_proj/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/attention/out_proj/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/layer_norm/gamma:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/layer_norm/beta:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/final_layer_norm/gamma:0', 'tf_wav2_vec2_for_ctc/wav2vec2/encoder/layers.17/final_layer_norm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Marwin Jay\\AppData\\Local\\Temp\\ipykernel_20904\\1761028653.py\", line 27, in train_step\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1223, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 638, in apply_gradients\n        self.build(trainable_variables)\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\optimizers\\adam.py\", line 145, in build\n        self.add_variable_from_reference(\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1125, in add_variable_from_reference\n        return super().add_variable_from_reference(\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 508, in add_variable_from_reference\n        initial_value = tf.zeros(\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\tensorflow\\dtensor\\python\\api.py\", line 64, in call_with_layout\n        return fn(*args, **kwargs)\n\n    ResourceExhaustedError: {{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[4096,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Fill] name: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mCTCModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Marwin Jay\\AppData\\Local\\Temp\\ipykernel_20904\\1761028653.py\", line 27, in train_step\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1223, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 638, in apply_gradients\n        self.build(trainable_variables)\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\optimizers\\adam.py\", line 145, in build\n        self.add_variable_from_reference(\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1125, in add_variable_from_reference\n        return super().add_variable_from_reference(\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 508, in add_variable_from_reference\n        initial_value = tf.zeros(\n    File \"c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\tensorflow-asr\\venv\\lib\\site-packages\\tensorflow\\dtensor\\python\\api.py\", line 64, in call_with_layout\n        return fn(*args, **kwargs)\n\n    ResourceExhaustedError: {{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[4096,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Fill] name: \n"
     ]
    }
   ],
   "source": [
    "history = CTCModel.fit(dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
